{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import csv\n",
    "import torch.nn.functional as F\n",
    "from torch import \n",
    "from torch import optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = h5py.File('X_train_new.h5', 'r')\n",
    "f_test = h5py.File('X_test_new.h5', 'r')\n",
    "\n",
    "X_train = f_train['features']\n",
    "X_test = f_test['features']\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "X_test = np.asarray(X_test)\n",
    "\n",
    "print('X train shape: ', X_train.shape)\n",
    "print('X_test_shape', X_test.shape)\n",
    "\n",
    "y_train = [0] * 946\n",
    "\n",
    "with open('y_train_AvCsavx.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "       # print(row['id'], row['label'])\n",
    "       y_train[int(row['id'])] = int(row['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_segm = np.reshape(X_train, [X_train.shape[0] * X_train.shape[1], 1, X_train.shape[3], X_train.shape[2]])\n",
    "X_train_segm = torch.from_numpy(X_train_segm)\n",
    "nr_train_examples = X_train_segm.shape[0]\n",
    "print(\"Number of training examples: \", str(nr_train_examples))\n",
    "\n",
    "\n",
    "# train/val split\n",
    "X_train_splitted = X_train_segm[:-( nr_train_examples // 10)]\n",
    "X_val_splitted = X_train_segm[-(nr_train_examples // 10):]\n",
    "\n",
    "print('X train after split: ', X_train_splitted.shape)\n",
    "print('X val after split: ', X_train_splitted.shape)\n",
    "\n",
    "\n",
    "# data normalization\n",
    "mean = torch.mean(X_train_splitted, (0, 1, 2), keepdim=True)\n",
    "std = torch.std(X_train_splitted, (0, 1, 2), keepdim=True)\n",
    "X_train_splitted = (X_train_splitted - mean) / (std + 1e-8)\n",
    "X_train_splitted = X_train_splitted.to(device=device)\n",
    "\n",
    "X_val_splitted = (X_val_splitted - mean) / (std + 1e-8)\n",
    "X_val_splitted = X_val_splitted.to(device=device)\n",
    "\n",
    "\n",
    "# extend y_train labels to every segment (instead of every patient)\n",
    "y_train_segm = np.repeat(y_train, 40)\n",
    "y_train_segm = torch.from_numpy(y_train_segm)\n",
    "\n",
    "y_train_splitted = y_train_segm[:-(nr_train_examples // 10)].to(device=device)\n",
    "y_val_splitted = y_train_segm[-(nr_train_examples // 10):].to(device=device)\n",
    "\n",
    "nr_cls_1_train = torch.sum(y_train_splitted).item()\n",
    "nr_cls_0_train =  y_train_splitted.shape[0] - nr_cls_1_train\n",
    "\n",
    "print(\"Number of examples of class 1 = \", nr_cls_1_train)\n",
    "print(\"Number of examples of class 0 = \", nr_cls_0_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversample from the minority class\n",
    "X_train_splitted = X_train_splitted.cpu().numpy()\n",
    "y_train_splitted = y_train_splitted.cpu().numpy()\n",
    "\n",
    "n_cls1 = np.sum(y_train_splitted)\n",
    "n_cls0 = len(y_train_splitted) - np.sum(y_train_splitted)\n",
    "diff_segm = n_cls0 - n_cls1\n",
    "\n",
    "\n",
    "# sample with replacement diff_segm times from the class 1 segm samples\n",
    "indices_cls1_splitted = np.where(np.asarray(y_train_splitted)==1)[0]\n",
    "indices_cls1_splitted_oversampled = np.random.choice(indices_cls1_splitted, size=diff_segm, replace=True)\n",
    "X_train_cls1_splitted_oversampled = X_train_splitted[indices_cls1_splitted_oversampled]\n",
    "\n",
    "\n",
    "# no Gaussian noise added\n",
    "X_train_cls1_splitted_oversampled_noise = X_train_cls1_splitted_oversampled #+ np.random.normal(0., 1.5, X_train_cls1_splitted_oversampled.shape)\n",
    "y_train_cls1_splitted_oversampled = [1] * diff_segm\n",
    "\n",
    "\n",
    "X_train_concat = np.vstack((X_train_splitted, X_train_cls1_splitted_oversampled_noise))\n",
    "y_train_concat = np.concatenate([np.asarray(y_train_splitted), np.asarray(y_train_cls1_splitted_oversampled)])\n",
    "print('X train after concat: ', X_train_concat.shape)\n",
    "print('y train after concat: ', y_train_concat.shape)\n",
    "\n",
    "\n",
    "# convert to torch tensors\n",
    "X_train_concat = torch.from_numpy(X_train_concat).type(torch.FloatTensor).to(device = device)\n",
    "y_train_concat = torch.from_numpy(y_train_concat).to(device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_CNN_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "                \n",
    "        self.conv1 = nn.Conv2d(1, 100, (5, 3), padding=(0, 1)) #(3, 3)\n",
    "        self.pool1 = nn.MaxPool2d((4, 1)) #(2, 2)\n",
    "        self.drop1 = nn.Dropout(0.25)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(100, 100, (5, 3), padding=(0, 1)) #(3, 3)\n",
    "        self.pool2 = nn.MaxPool2d((4, 1)) #(2, 2)\n",
    "        self.drop2 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(100, 100, (9, 3), padding=(0, 1))  #(2, 3)\n",
    "        self.pool3 = nn.MaxPool2d((10, 1)) #(2, 2)\n",
    "        \n",
    "        self.fc = nn.Linear(100 * 2 * 7, 2, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "                \n",
    "        x = self.drop1(self.pool1(F.relu(self.conv1(x))))\n",
    "        x = self.drop2(self.pool2(F.relu(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "     \n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_CNN_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 100, (5, 3), padding=(0, 1))\n",
    "        self.pool1 = nn.MaxPool2d((4, 1))\n",
    "        self.drop1 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(100, 100, (5, 3), padding=(0, 1)) \n",
    "        self.pool2 = nn.MaxPool2d((4, 1))\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(100, 100, (9, 3), padding=(0, 1)) \n",
    "        self.pool3 = nn.MaxPool2d((10, 1))\n",
    "        \n",
    "        self.fc = nn.Linear(100 * 2 * 7, 2, bias = True)\n",
    "\n",
    "    def forward(self, x):\n",
    "         \n",
    "        x = self.drop1(self.pool1(F.relu(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.conv2(x))) \n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        \n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "\n",
    "net = Net_CNN_2()\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.0001)\n",
    "\n",
    "batch_size = 64\n",
    "print_every_n = 500\n",
    "\n",
    "for epoch in range(150):\n",
    "        \n",
    "    perm = np.random.permutation(X_train_concat.shape[0])\n",
    "    perm = torch.from_numpy(perm).to(device=device)\n",
    "    \n",
    "    X_train_concat = X_train_concat[perm]#.to(device)\n",
    "    y_train_concat = y_train_concat[perm]#.to(device)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    acc_print = 0.0\n",
    "    \n",
    "    for batch_i in range(X_train_concat.shape[0] // batch_size):\n",
    "        inputs = X_train_concat[batch_i * batch_size:(batch_i+1) * batch_size]\n",
    "        labels = y_train_concat[batch_i * batch_size:(batch_i+1) * batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "                \n",
    "        loss = torch.mean(criterion(outputs, labels))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pred = outputs.data.max(1, keepdim=True)[1]\n",
    "        correct = pred.eq(labels.view_as(pred)).cuda().sum()\n",
    "        acc = ((correct * (1.0)) / ((1.0) * labels.shape[0]))\n",
    "        total_acc += acc.item()\n",
    "        acc_print += acc.item()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if batch_i % print_every_n == 0:   \n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, batch_i + 1, running_loss / print_every_n))\n",
    "\n",
    "            print('[%d, %5d] acc: %.3f' %\n",
    "                  (epoch + 1, batch_i + 1, acc_print / print_every_n))           \n",
    "            \n",
    "            running_loss = 0.0\n",
    "            acc_print = 0.0\n",
    "    \n",
    "    print(\"Accuracy train epoch \", epoch, \": \", total_acc /  (X_train_concat.shape[0] // batch_size))\n",
    "\n",
    "    total_acc_val = 0.0\n",
    "    total_loss_val = 0.0\n",
    "    batch_size_val = 16\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_i in range(X_val_splitted.shape[0] // batch_size_val):\n",
    "\n",
    "            inputs_val = X_val_splitted[batch_i * batch_size_val:(batch_i+1) * batch_size_val]#.to(device)\n",
    "            labels_val = y_val_splitted[batch_i * batch_size_val:(batch_i+1) * batch_size_val]#.to(device)\n",
    "            outputs_val = net(inputs_val)\n",
    "            \n",
    "            outputs_val = F.softmax(outputs_val, dim = 1)\n",
    "\n",
    "            '''\n",
    "            outputs_val[:, 0] = outputs_val[:, 0] * ((nr_cls_0_train * 1.0) + nr_cls_1_train) / (1.0 * nr_cls_0_train)\n",
    " \n",
    "            outputs_val[:, 1] = outputs_val[:, 1] * ((nr_cls_0_train * 1.0) + nr_cls_1_train) / (1.0 * nr_cls_1_train)\n",
    "    \n",
    "                    \n",
    "            temp = outputs_val[:, 0]\n",
    "            \n",
    "            outputs_val[:, 0] = temp / (temp + outputs_val[:, 1])\n",
    "            \n",
    "            outputs_val[:, 1] = 1 - outputs_val[:, 0] \n",
    "            \n",
    "            '''\n",
    "            \n",
    "            if batch_i%100 == 0:       \n",
    "                print(outputs_val[:5])\n",
    "            \n",
    "            pred = outputs_val.data.max(1, keepdim=True)[1]\n",
    "            correct = pred.eq(labels_val.view_as(pred)).cuda().sum()\n",
    "            acc_val = ((correct * (1.0)) / ((1.0) * labels_val.shape[0]))\n",
    "            loss_val = torch.mean(criterion(outputs_val, labels_val)).item()\n",
    "            total_acc_val += acc_val\n",
    "            total_loss_val += loss_val\n",
    "\n",
    "        print(\"Accuracy val epoch: \", epoch, \": \", (total_acc_val /  (X_val_splitted.shape[0] // batch_size_val)))\n",
    "        print(\"Loss val epoch \", epoch, \": \", total_loss_val / (X_val_splitted.shape[0] // batch_size_val))\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_segm = torch.from_numpy(np.reshape(X_test, [X_test.shape[0] * X_test.shape[1], 1, X_test.shape[3], X_test.shape[2]]))\n",
    "X_test_segm = (X_test_segm - mean) / (std + 1e-8)\n",
    "X_test_segm = X_test_segm.to(device)\n",
    "\n",
    "batch_size = 40\n",
    "preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_i in range(X_test_segm.shape[0] // batch_size):\n",
    "        inputs = X_test_segm[batch_i * batch_size:(batch_i+1) * batch_size]\n",
    "        outputs_test = F.softmax(net(inputs), dim=1)\n",
    "\n",
    "        '''\n",
    "        outputs_test[:, 0] = outputs_test[:, 0] * ((nr_cls_0_train * 1.0) + nr_cls_1_train) / (1.0 * nr_cls_0_train)\n",
    "\n",
    "        outputs_test[:, 1] = outputs_test[:, 1] * ((nr_cls_0_train * 1.0) + nr_cls_1_train) / (1.0 * nr_cls_1_train)\n",
    "\n",
    "        temp = outputs_test[:, 0]\n",
    "\n",
    "        outputs_test[:, 0] = temp / (temp + outputs_test[:, 1])\n",
    "\n",
    "        outputs_test[:, 1] = 1 - outputs_test[:, 0]        \n",
    "        '''\n",
    "        \n",
    "        preds += outputs_test.data[:, 1]\n",
    "    \n",
    "    \n",
    "preds = np.asarray(preds)\n",
    "preds = np.reshape(preds, (-1, 40))\n",
    "preds = np.mean(preds, axis=1)\n",
    "\n",
    "preds = (np.sign(preds-0.5) + 1)//2\n",
    "preds = [int(i) for i in preds]\n",
    "\n",
    "out = open('CNN_150epochs_oversampling_noThreshold.csv', \"w\")\n",
    "out.write(\"id,label\\n\")\n",
    "rows = ['']*len(preds)\n",
    "for num in range(len(preds)):\n",
    "    rows[num]='%d,%d\\n' % (num, preds[num])\n",
    "out.writelines(rows)\n",
    "out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
