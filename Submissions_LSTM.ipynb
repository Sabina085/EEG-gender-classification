{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import pywt\n",
    "import csv\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = h5py.File('X_train_new.h5', 'r')\n",
    "f_test = h5py.File('X_test_new.h5', 'r')\n",
    "X_train = f_train['features']\n",
    "X_test = f_test['features']\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "X_test = np.asarray(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [0] * 946\n",
    "\n",
    "with open('y_train_AvCsavx.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "       # print(row['id'], row['label'])\n",
    "       y_train[int(row['id'])] = int(row['label'])\n",
    "    \n",
    "    \n",
    "#extend y_train labels to every segment (instead of every patient)\n",
    "y_train_segm = np.repeat(y_train, 40)\n",
    "y_train_segm = torch.from_numpy(y_train_segm)\n",
    "X_train_sequences = np.reshape(X_train, (X_train.shape[0] * X_train.shape[1] * X_train.shape[2], X_train.shape[3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction using wavelets\n",
    "\n",
    "preprocessed = []\n",
    "\n",
    "for i in range(X_train_sequences.shape[0]):\n",
    "    x = X_train_sequences[i]\n",
    "    wp = pywt.WaveletPacket(data=x, wavelet='db8', mode='symmetric')\n",
    "    preprocessed.append(wp['aaad'].data)\n",
    "\n",
    "\n",
    "preprocessed = np.asarray(preprocessed)\n",
    "X_train_segm = np.reshape(preprocessed, (X_train.shape[0] * X_train.shape[1], X_train.shape[2], \n",
    "                                         preprocessed.shape[1]))\n",
    "                    \n",
    "X_train_segm = np.transpose(X_train_segm, (0, 2, 1))\n",
    "X_train_segm = torch.from_numpy(X_train_segm)\n",
    "print('X_train_segm shape', X_train_segm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_segm = np.reshape(X_train, [X_train.shape[0] * X_train.shape[1], 1, X_train.shape[3], X_train.shape[2]])\n",
    "\n",
    "# train/val split\n",
    "nr_train_examples = X_train_segm.shape[0]\n",
    "print('nr train examples', nr_train_examples)\n",
    "X_train_splitted = X_train_segm[:-( nr_train_examples // 10)]\n",
    "X_val_splitted = X_train_segm[-(nr_train_examples // 10):]\n",
    "\n",
    "\n",
    "# data normalization\n",
    "mean = torch.mean(X_train_splitted, (0, 1), keepdim=True)\n",
    "std = torch.std(X_train_splitted, (0, 1), keepdim=True)\n",
    "X_train_splitted = (X_train_splitted - mean) / (std + 1e-8)\n",
    "X_train_splitted = X_train_splitted.to(device=device)\n",
    "X_val_splitted = (X_val_splitted - mean) / (std + 1e-8)\n",
    "X_val_splitted = X_val_splitted.to(device=device)\n",
    "print('X_train_splitted.shape', X_train_splitted.shape)\n",
    "print('X_val_splitted.shape', X_val_splitted.shape)\n",
    "\n",
    "\n",
    "y_train_splitted = y_train_segm[:-(nr_train_examples // 10)].to(device=device)\n",
    "y_val_splitted = y_train_segm[-(nr_train_examples // 10):].to(device=device)\n",
    "print('y_train_splitted.shape', y_train_splitted.shape)\n",
    "print('y_val_splitted.shape', y_val_splitted.shape)\n",
    "\n",
    "\n",
    "X_train_concat = X_train_splitted #torch.from_numpy(X_train_splitted).to(device=device)\n",
    "y_train_concat = y_train_splitted #torch.from_numpy(y_train_splitted).to(device=device)\n",
    "\n",
    "nr_cls_1_train = torch.sum(y_train_concat).item()\n",
    "nr_cls_0_train =  y_train_concat.shape[0] - nr_cls_1_train\n",
    "\n",
    "print(\"Nr_cls_1_train = \", nr_cls_1_train)\n",
    "print(\"Nr_cls_0_train = \", nr_cls_0_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_EEG(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, n_layers, drop_prob=0.):\n",
    "        super(LSTM_EEG, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, dropout=drop_prob, \n",
    "                            batch_first=True, bidirectional=False)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, 2, bias=True)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden_state = torch.randn(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        cell_state = torch.randn(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        hidden = (hidden_state, cell_state)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "\n",
    "net = LSTM_EEG(input_size = X_train_segm.shape[2], hidden_dim=128, n_layers=2)\n",
    "\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.005)\n",
    "\n",
    "batch_size = 64 #32\n",
    "print_every_n = 200\n",
    "\n",
    "for epoch in range(100):  \n",
    "    perm = np.random.RandomState(seed=42).permutation(X_train_concat.shape[0])    \n",
    "    perm = torch.from_numpy(perm).to(device=device)\n",
    "    \n",
    "    X_train_concat = X_train_concat[perm]#.to(device)\n",
    "    y_train_concat = y_train_concat[perm]#.to(device)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    acc_print = 0.0\n",
    "    \n",
    "    for batch_i in range(X_train_concat.shape[0] // batch_size):\n",
    "        inputs = X_train_concat[batch_i * batch_size:(batch_i+1) * batch_size]\n",
    "        labels = y_train_concat[batch_i * batch_size:(batch_i+1) * batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hidden = net.init_hidden(batch_size)\n",
    "        outputs = net(inputs, hidden)\n",
    "                \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.)\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        pred = outputs.data.max(1, keepdim=True)[1]\n",
    "        correct = pred.eq(labels.view_as(pred)).cuda().sum()\n",
    "        acc = ((correct * (1.0)) / ((1.0) * labels.shape[0]))\n",
    "        total_acc += acc.item()\n",
    "        acc_print += acc.item()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if batch_i % print_every_n == 0:   \n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, batch_i + 1, running_loss / print_every_n))\n",
    "\n",
    "            print('[%d, %5d] acc: %.3f' %\n",
    "                  (epoch + 1, batch_i + 1, acc_print / print_every_n))           \n",
    "            \n",
    "            running_loss = 0.0\n",
    "            acc_print = 0.0\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    print(\"Accuracy train epoch \", epoch, \": \", total_acc /  (X_train_concat.shape[0] // batch_size))\n",
    "\n",
    "    total_acc_val = 0.0\n",
    "    total_loss_val = 0.0\n",
    "    batch_size_val = 16\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_i in range(X_val_splitted.shape[0] // batch_size_val):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs_val = X_val_splitted[batch_i * batch_size_val:(batch_i+1) * batch_size_val]#.to(device)\n",
    "            labels_val = y_val_splitted[batch_i * batch_size_val:(batch_i+1) * batch_size_val]#.to(device)\n",
    "            \n",
    "            hidden = net.init_hidden(batch_size_val)\n",
    "            outputs_val = net(inputs_val, hidden)            \n",
    "            \n",
    "            '''\n",
    "            # Thresholding\n",
    "            \n",
    "            outputs_val = F.softmax(outputs_val, dim = 1)\n",
    "            \n",
    "            outputs_val[:, 0] = outputs_val[:, 0] * ((nr_cls_0_train * 1.0) + nr_cls_1_train) / (1.0 * nr_cls_0_train)\n",
    " \n",
    "            outputs_val[:, 1] = outputs_val[:, 1] * ((nr_cls_0_train * 1.0) + nr_cls_1_train) / (1.0 * nr_cls_1_train)\n",
    "    \n",
    "                    \n",
    "            temp = outputs_val[:, 0]\n",
    "            \n",
    "            outputs_val[:, 0] = temp / (temp + outputs_val[:, 1])\n",
    "            \n",
    "            outputs_val[:, 1] = 1 - outputs_val[:, 0] \n",
    "            \n",
    "            \n",
    "            print('outputs_val', outputs_val[0:10])\n",
    "            '''\n",
    "            \n",
    "            pred = outputs_val.data.max(1, keepdim=True)[1]\n",
    "            \n",
    "            print('pred: ', pred[:10])\n",
    "            \n",
    "            correct = pred.eq(labels_val.view_as(pred)).cuda().sum()\n",
    "            acc_val = ((correct * (1.0)) / ((1.0) * labels_val.shape[0]))\n",
    "            loss_val = criterion(outputs_val, labels_val).item()\n",
    "            total_acc_val += acc_val\n",
    "            total_loss_val += loss_val\n",
    "\n",
    "        print(\"Accuracy val epoch: \", epoch, \": \", (total_acc_val /  (X_val_splitted.shape[0] // batch_size_val)))\n",
    "        print(\"Loss val epoch \", epoch, \": \", total_loss_val / (X_val_splitted.shape[0] // batch_size_val))\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sequences = np.reshape(X_test, (X_test.shape[0] * X_test.shape[1] * X_test.shape[2], X_test.shape[3]))\n",
    "\n",
    "preprocessed = []\n",
    "\n",
    "for i in range(X_test_sequences.shape[0]):\n",
    "    x = X_test_sequences[i]\n",
    "    wp = pywt.WaveletPacket(data=x, wavelet='db8', mode='symmetric')\n",
    "    preprocessed.append(wp['aaad'].data)\n",
    "\n",
    "    \n",
    "preprocessed = np.asarray(preprocessed)\n",
    "X_test_segm = np.reshape(preprocessed, (X_test.shape[0] * X_test.shape[1], X_test.shape[2], \n",
    "                                         preprocessed.shape[1]))\n",
    "                    \n",
    "X_test_segm = np.transpose(X_test_segm, (0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_segm = torch.from_numpy(X_test_segm)\n",
    "X_test_segm = (X_test_segm - mean) / (std + 1e-8)\n",
    "X_test_segm = X_test_segm.to(device)\n",
    "\n",
    "batch_size = 40\n",
    "preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_i in range(X_test_segm.shape[0] // batch_size):\n",
    "        inputs = X_test_segm[batch_i * batch_size:(batch_i+1) * batch_size]\n",
    "\n",
    "        hidden = net.init_hidden(batch_size)\n",
    "\n",
    "        outputs_test = F.softmax(net(inputs, hidden), dim=1) #added item()\n",
    "        \n",
    "        '''\n",
    "        outputs_test[:, 0] = outputs_test[:, 0] * ((nr_cls_0_train * 1.0) + nr_cls_1_train) / (1.0 * nr_cls_0_train)\n",
    "\n",
    "        outputs_test[:, 1] = outputs_test[:, 1] * ((nr_cls_0_train * 1.0) + nr_cls_1_train) / (1.0 * nr_cls_1_train)\n",
    "\n",
    "        temp = outputs_test[:, 0]\n",
    "\n",
    "        outputs_test[:, 0] = temp / (temp + outputs_test[:, 1])\n",
    "\n",
    "        outputs_test[:, 1] = 1 - outputs_test[:, 0]\n",
    "        '''\n",
    "        \n",
    "        preds += outputs_test.data[:, 1]\n",
    "    \n",
    "\n",
    "preds = np.asarray(preds)\n",
    "preds = np.reshape(preds, (-1, 40))\n",
    "preds = np.mean(preds, axis=1)\n",
    "preds = (np.sign(preds-0.5) + 1)//2\n",
    "preds = [int(i) for i in preds]\n",
    "\n",
    "out = open('LSTM_baseline.csv', \"w\")\n",
    "out.write(\"id,label\\n\")\n",
    "rows = ['']*len(preds)\n",
    "for num in range(len(preds)):\n",
    "    rows[num]='%d,%d\\n' % (num, preds[num])\n",
    "out.writelines(rows)\n",
    "out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
